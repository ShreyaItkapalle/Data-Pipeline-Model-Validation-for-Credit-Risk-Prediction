# ğŸ“Š Data Pipeline & Model Validation for Credit Risk Prediction

## ğŸ“Œ Project Overview
This project implements an **end-to-end data pipeline and model validation workflow** for credit risk prediction using structured financial data. The objective is to ensure **data reliability, validation, and reproducibility** before deploying a machine learning model in a real-world business environment.

The project simulates how data and risk analytics teams validate datasets and models before using them for **credit decisioning**.

---

## ğŸ¯ Business Objective
To predict the likelihood of customer default while ensuring:
- High data quality and integrity  
- Compliance with business validation rules  
- Reliable and interpretable model performance  

---

## ğŸ› ï¸ Technology Stack
- **Language:** Python  
- **Libraries:** Pandas, NumPy, Scikit-learn, Joblib  
- **Environment:** Google Colab  
- **Storage:** Google Drive  
- **Version Control:** GitHub  

---

## ğŸ“‚ Project Structure
data-pipeline-model-validation/
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/
â”‚ â”‚ â””â”€â”€ credit_data.csv
â”‚ â””â”€â”€ processed/
â”‚ â””â”€â”€ clean_credit_data.csv
â”‚
â”œâ”€â”€ notebooks/
â”‚ â””â”€â”€ data_pipeline_model_validation.ipynb
â”‚
â”œâ”€â”€ models/
â”‚ â””â”€â”€ credit_risk_model.pkl
â”‚
â”œâ”€â”€ reports/
â”‚ â””â”€â”€ model_evaluation.txt
â”‚
â””â”€â”€ README.md

---

## ğŸ”„ Project Workflow

### 1ï¸âƒ£ Data Ingestion
- Loaded raw credit data from storage
- Verified schema, data types, and column consistency

### 2ï¸âƒ£ Data Quality Checks
- Checked missing values, duplicates, and invalid entries
- Identified unrealistic and undefined categorical values

### 3ï¸âƒ£ Data Cleaning & Preparation
- Handled invalid values in age and categorical fields
- Applied appropriate imputation techniques
- Generated a clean, analysis-ready dataset

### 4ï¸âƒ£ Data Validation Rules
- Applied business rules such as:
  - Valid age range
  - Non-negative financial values
  - Domain constraints for categorical variables

### 5ï¸âƒ£ Model Training
- Trained a Logistic Regression model for default prediction
- Used stratified train-test split to handle class imbalance

### 6ï¸âƒ£ Model Validation
- Evaluated model using accuracy, precision, recall, and F1-score
- Analyzed misclassification impact from a business perspective

### 7ï¸âƒ£ Model Persistence
- Saved trained model using `joblib`
- Stored model artifact for reuse and deployment readiness

---

## ğŸ“ˆ Model Performance Summary
Key evaluation metrics are documented in: reports/model_evaluation.txt
This includes classification metrics used to validate model reliability.

---

## ğŸ’¾ Saved Artifacts
- **Processed Data:** `data/processed/clean_credit_data.csv`
- **Trained Model:** `models/credit_risk_model.pkl`
- **Evaluation Report:** `reports/model_evaluation.txt`

---

## ğŸ“Œ Key Learnings
- Importance of data validation before modeling
- Building reproducible and auditable data pipelines
- Applying business rules to analytical workflows
- Model evaluation and persistence best practices

---

## ğŸš€ Future Enhancements
- Implement cross-validation and hyperparameter tuning
- Automate pipeline using sklearn Pipelines
- Deploy model using Flask or FastAPI
- Monitor data drift and model performance

---

## ğŸ‘©â€ğŸ’» Author
**Shreya Itkapalle**  
Aspiring Data Analyst / Data Scientist  

---
